Hi, Today I will show you how to write a waveform that will danse with the music. I prepared a small skeleton that is just composed of an audio element with a mp3 file that will be streamed when I press the play button. The sound is captured in an audio graph using a media element source like we explained in a previous lesson. So let's start from the beginning and look at this example. When the page is loaded we go to the onload listener, we create an audio context because we are going to work with the Web Audio API, and then we are just using the standard way for using a canvas. We get the canvas, and we get the canvas context, so we are ready to draw things in the canvas here. Then we build an audio graph and then we start an animation... and for the moment the animation just draws an horizontal line 60 times per seconds. We will look at it later. Let's have a look at the audio graph. The audio graph is made of a source node that is the media element source. It corresponds to the audio element. Then we create an analyser, this is a special node that will provide on demand the time domain and frequency domain analysis data, and this data will be useful for drawing a waveform or for drawing frequencies that will dance with the music, or for drawing volume meters, or for doing beat detection. We will see examples in the next lessons. When you create an analyser node, you specify the size of the Fast Fourier Transform (FFT), do not worry if you do not know exactly what this technique is. It is just the size that will have an influence on the number of data you will have to draw. Here for waveform, classical values are 1024 or 2048. They must be powers of 2. The number of data we will get depends on this size:  it is exactly the FFT size divided by 2. In this example I set the FFT size to 1024 so I will get 512 data... So here is how we can declare a buffer that will get the data. It is called dataArray here and we will use it in the animation loop. So, I've got a source node that corresponds to the audio stream, an analyser node that will analyse the stream, and then I connect the source to the analyser, and the analyser to the destination, and the destination is the speakers. Let's have a look at the animation loop! We clear the canvas, and in the end we call again the animation loop, so that the animation will be done 60 times per seconds... and the way we will draw the waveform is just a set of connected line that we call "a path". The "path" is a way of drawing that we presented during the HTML5 Part 1 course. So here, for drawing an horizontal, flat waveform, we just do a loop on the number of data. Here, we are not using the real data but we will do a loop 512 times and we compute an increment in x, so that depending on the width of the canvas and on the number of data we have got to draw, we will add this increment to the x coordinate. And the y coordinate here is just faked because we use height/2. I can add a random element here if you like, and we will see that I can just fake some data to be drawn. So, instead of drawing this, I will use the real values. How can we get the data? In the animation loop, 60 times per seconds, we call the analyser.getByteTimeDomainData and we pass the dataArray of the correct size. And just after this call, the dataArray will contain the data we want to draw. And what is interesting here, is the value of each data... and we will compute the y coordinate depending on that. I'm just declaring a variable that will get the value. This value is between 0 and 255 because we are working with bytes, and bytes are 8 bits encoded data and the value is between 0 and 255. I will normalize it. So now it's between 0 and 1. Then, in order to compute the y value, I just have to scale it to the height of the canvas, like this... And now if I play the sound, I've got the waveform that is animated. These 3 lines are very straightforward, just in order to transform a value between 0 and 255 and scale it to the height of the canvas...