in this video we'll define something called the cost function this will let us figure out how to fit the best possible straight line to our data in linear regression we have a training set like that shown here remember our notation M was the number of training examples so maybe M equals 47 and the form of our hypothesis which we use to make predictions is this linear function to introduce a little bit more terminology these theta 0 and theta 1 write these theta eyes are what I call the parameters of the model and what we're going to do in this video is talk about how to go about choosing these two parameter values theta 0 and theta 1 with different choices of the parameters theta 0 and theta 1 we get different hypotheses different hypothesis functions I know some of you will probably be already familiar with what I'm going to do on the slide but just a review here are a few examples if theta 0 is 1 point 5 and theta 1 is 0 then the hypothesis function will look like this and because your hypothesis function will be H of x equals 1 point 5 plus 0 times X which is this constant value function there's just fat at 1.5 if theta 0 equals 0 theta 1 equals 0.5 then the hypotheses will look like this and it should pass through this point to one since you now have H of X really H sub substrate theta of X but sometimes out well I'll just omit theta for gravity so H of X would be equal to just 0.5 times X which looks like that and finally if theta is equals 1 and theta 1 equals 0.5 then we end up with a hypothesis that looks like this let's see I should pass through the 2 to point like so and this is my you H of X on my new subscript theta of X all right where you remember I said that this is H subscript theta of X but as a shorthand sometimes I'll just write this as H of X in linear regression we have a training set like maybe the one I plotted here what we want to do is come up with values for the parameters theta zero and theta one so then the straight line we get all of this corresponds to a straight line that somehow fits the data well maybe that line over there so how do we come up with you know values theta 0 theta 1 that corresponds to a good fit to the data the idea is we're going to choose our parameters theta 0 theta 1 so that H of X meaning the value we predict on implies X that this is at least close to the values Y for the examples in our training set for our training examples so in our training set we're given a number of examples where we know X decides the house and we know the actual price that was so for so let's try to choose values to the parameters so that at least in the training set given the XS in the training set we make reasonably accurate predictions for the Y values let's formalize this sin linear regression what we're going to do is I'm going to want to solve a minimization problem so the write minimize over theta0 theta1 and I wanted this to be small right I want the difference between H of x and y to be small and one thing I might do is try to minimize the squared difference between the output of my hypothesis and the actual price of a house okay so let's throw in some details you remember that I was using the notation x I comma Y I to represent the I've training example so what I want really is to sum over my training set sum from I equals 1 to M of the squared difference between this is the prediction of my hypothesis when it is input the size of house number I right - the actual price that house number I was so full and I want to minimize the sum over my training set sum from I equals 1 through m of the difference of the squared error squared difference between the predicted price of the house and the price that it was actually so for and just remind you of your notation M here was the size of my training set right so little m there is the my number of training examples that hash sign is the abbreviation for number of training examples okay and to make some of our later map a little bit easier I'm going to actually look at you know 1 over m times AB so we'll try to minimize my average ever visually minimize 1 over 2m the putting the to the constant you know 1/2 in front it just makes some of the math mo bit easier so minimizing 1/2 of something right should give you the same values for the parameters theta 0 and theta 1 as minimizing that function and just make sure this this this equation is clear write this expression in here take subscript theta of X this is my this is our usual write that is equal to this plus beta 1 X I and this notation minimize over theta0 and theta1 this means you'll find me the values of theta 0 and theta 1 that causes this expression to be minimized and this expression depends on theta 0 and theta 1 okay so just to recap we're posing this problem as find me the values of theta 0 and theta one so that the average already 1 over 2m times the sum of squared errors between my predictions on the training set minus the actual values the houses on the training set is minimized so this is going to be my overall objective function for linear regression and just to rewrite this out a little bit more cleanly what I'm going to do is by convention we usually define a cost function which is going to be exactly this that formula that I have up here and what I want to do is minimize over theta0 and theta1 my function J of theta 0 comma theta 1 where just read this out this is my cost function so this cost function is also called the squared error function sometimes called the squared error cost function and it turns out that why do we take the squares of the errors it turns out that the squared error cost function is a reasonable choice and will work well for most problems for most regression problems there are other cost functions that will work pretty well but the squared error cost function is probably the most commonly used one for regression problems layton's is also talked about alternative cost functions as well but this this choice that we just had should be a pretty reasonable thing to try for most linear regression problems okay so that's the cost function so far we've just seen a mathematical definition of you know this cost function and in case this this function j of theta0 theta1 in case this function seems a little bit abstract and you still don't have a good sense of what is doing in the next video in the next couple of videos much going to go a little bit deeper into what the cost function J is doing and trying to give you better intuition about what is computing and why we want to use it