WEBVTT - https://subtitletools.com

00:00:00.920 --> 00:00:04.500
>> Classical machine learning can take you so far,

00:00:04.500 --> 00:00:06.120
but all the cool kids are getting into

00:00:06.120 --> 00:00:09.240
so-called deep learning. So let's take a look at that.

00:00:09.240 --> 00:00:12.120
As you know, machine learning is fundamentally about

00:00:12.120 --> 00:00:15.090
defining a function that produces a label value,

00:00:15.090 --> 00:00:18.660
y, from a set of features, x.

00:00:18.660 --> 00:00:22.110
Now the features are usually numeric values,

00:00:22.110 --> 00:00:25.305
like the measurements of an iris flower's petals and sepals,

00:00:25.305 --> 00:00:29.550
and in classification, the label is a probability vector that

00:00:29.550 --> 00:00:31.530
identifies the most likely class to which

00:00:31.530 --> 00:00:34.020
the item described by the features belongs.

00:00:34.020 --> 00:00:36.060
Classical machine learning techniques use

00:00:36.060 --> 00:00:37.950
algorithms like logistic regression,

00:00:37.950 --> 00:00:40.465
to determine coefficients for the function.

00:00:40.465 --> 00:00:43.310
We're going to look at a different approach

00:00:43.310 --> 00:00:46.110
that tries to mimic the human brain.

00:00:46.150 --> 00:00:49.490
In your brain, you have neurons,

00:00:49.490 --> 00:00:52.550
and attached to these neurons are nerve tissues called

00:00:52.550 --> 00:00:54.650
dendrites that contain synapses

00:00:54.650 --> 00:00:57.725
that capture input signals for the neuron.

00:00:57.725 --> 00:01:01.505
As you learn to respond to specific stimuli,

00:01:01.505 --> 00:01:04.940
these nerve impulses are strengthened or weakened.

00:01:04.940 --> 00:01:08.935
If the input signal is strong enough, the neuron fires,

00:01:08.935 --> 00:01:11.270
passing the signal onto other neurons in

00:01:11.270 --> 00:01:14.675
the neural network that forms your central nervous system.

00:01:14.675 --> 00:01:17.660
Then conceptually, you can think of

00:01:17.660 --> 00:01:20.995
this biological arrangement is being like a mathematical function.

00:01:20.995 --> 00:01:23.420
The impulses received by the neurons

00:01:23.420 --> 00:01:25.820
are input values which we'll call x.

00:01:25.820 --> 00:01:27.200
In this case there's a vector of

00:01:27.200 --> 00:01:30.275
2x values that we'll call x1 and x2.

00:01:30.275 --> 00:01:32.825
Whether or not the neuron fires,

00:01:32.825 --> 00:01:35.590
is our output value or y.

00:01:35.590 --> 00:01:38.600
We can also view the strength or weakness of

00:01:38.600 --> 00:01:40.760
the input signal as being something that's

00:01:40.760 --> 00:01:43.250
used to assign weight to each input value,

00:01:43.250 --> 00:01:47.450
so we will call this w. All of that gives us

00:01:47.450 --> 00:01:52.470
a function that operates on x and w to produce y.

00:01:52.870 --> 00:01:55.985
Because this is an artificial neuron,

00:01:55.985 --> 00:01:57.890
we want to have some additional control

00:01:57.890 --> 00:01:59.795
over whether or not the neuron fires.

00:01:59.795 --> 00:02:04.190
So we actually add an additional input that we call bias or b.

00:02:04.190 --> 00:02:07.115
So our function now operates on x,

00:02:07.115 --> 00:02:10.045
w, and b, to produce y.

00:02:10.045 --> 00:02:12.885
But what does it actually do?

00:02:12.885 --> 00:02:16.940
Well, at the core, it calculates a weighted sum

00:02:16.940 --> 00:02:20.495
of each x input multiplied by its corresponding weight,

00:02:20.495 --> 00:02:23.060
and adds the bias value.

00:02:23.060 --> 00:02:27.540
This can produce theoretically any numerical value for y.

00:02:27.540 --> 00:02:31.580
We want to simulate a neuron which fires or doesn't,

00:02:31.580 --> 00:02:34.940
depending on whether or not a specific threshold is reached.

00:02:34.940 --> 00:02:38.570
So we wrap the weighted sum in a function

00:02:38.570 --> 00:02:41.809
that squashes the output value within a specific range,

00:02:41.809 --> 00:02:43.640
often a sigmoid function that

00:02:43.640 --> 00:02:46.215
yields a value between between zero and one.

00:02:46.215 --> 00:02:49.270
We call this function and activation function,

00:02:49.270 --> 00:02:51.080
and it's what determines whether or not

00:02:51.080 --> 00:02:53.670
our artificial neuron fires.

00:02:55.390 --> 00:02:57.980
So how does this artificial neuron

00:02:57.980 --> 00:03:00.985
function work in a machine learning model?

00:03:00.985 --> 00:03:03.780
Well, let's take some input values,

00:03:03.780 --> 00:03:06.500
in this case a vector containing the sepal and petal,

00:03:06.500 --> 00:03:08.750
width and height measurements for an iris,

00:03:08.750 --> 00:03:11.810
and we'll create a neuron for each input.

00:03:11.810 --> 00:03:14.030
So we're passing 1x value,

00:03:14.030 --> 00:03:17.990
with a randomly generated weight and bias value into each neuron.

00:03:17.990 --> 00:03:20.030
We're calculating a weighted sum,

00:03:20.030 --> 00:03:22.115
and we're using an activation function

00:03:22.115 --> 00:03:25.030
to squeeze the result between zero and one.

00:03:25.030 --> 00:03:27.825
Then the neurons in this input layer

00:03:27.825 --> 00:03:30.740
pass their outputs onto another layer of neurons,

00:03:30.740 --> 00:03:32.600
with each neuron in the first layer being

00:03:32.600 --> 00:03:35.165
connected to every neuron in the second layer.

00:03:35.165 --> 00:03:39.780
Again, the weights and bias values are randomly initialized.

00:03:39.940 --> 00:03:42.410
We can have as many of these layers as we

00:03:42.410 --> 00:03:44.210
like, we call them hidden layers,

00:03:44.210 --> 00:03:47.750
until finally we connect the outputs to an output layer,

00:03:47.750 --> 00:03:49.790
that has the same number of neurons as we have

00:03:49.790 --> 00:03:52.715
classes to which we are trying to assign the observed features.

00:03:52.715 --> 00:03:54.455
In this case, three classes

00:03:54.455 --> 00:03:57.410
representing the three possible species of virus.

00:03:57.410 --> 00:04:00.350
The output from the final output layer

00:04:00.350 --> 00:04:02.990
contains a probability value for each class,

00:04:02.990 --> 00:04:05.810
which we can interpret as a vector.

00:04:05.810 --> 00:04:09.410
Now this whole network of fully-connected layers is called a

00:04:09.410 --> 00:04:13.250
neural network or sometimes a multilayer perceptron.

00:04:13.250 --> 00:04:17.360
Now, the clever bit is that because we're

00:04:17.360 --> 00:04:19.275
training this network with some iris data

00:04:19.275 --> 00:04:21.310
where we already know the correct classes,

00:04:21.310 --> 00:04:24.050
we can compare the probability values that are

00:04:24.050 --> 00:04:28.060
generated by the network to the actual known values.

00:04:28.060 --> 00:04:29.960
In this case, the features in

00:04:29.960 --> 00:04:33.245
the input layer are measurements of a Virginica iris,

00:04:33.245 --> 00:04:35.405
which is the second of our three classes.

00:04:35.405 --> 00:04:40.170
So the correct probability should be zero, one, and zero.

00:04:40.180 --> 00:04:43.430
Now we can measure how far off each of

00:04:43.430 --> 00:04:47.680
our predicted probabilities are from the actual grown truth label.

00:04:47.680 --> 00:04:51.620
Then we'll take the mean of that, and we'll square it.

00:04:51.620 --> 00:04:53.840
That gives us a metric that

00:04:53.840 --> 00:04:55.670
indicates the amount of error in the model,

00:04:55.670 --> 00:04:57.690
which we call as loss.

00:04:58.450 --> 00:05:01.310
So now that we know how to calculate the loss in

00:05:01.310 --> 00:05:03.850
the model, what can we do with it?

00:05:03.850 --> 00:05:07.370
Well, the calculation for the loss is

00:05:07.370 --> 00:05:09.290
a function which takes as

00:05:09.290 --> 00:05:11.735
input the output layer of the neural network,

00:05:11.735 --> 00:05:14.510
which in turn can be thought of as a function that operates on

00:05:14.510 --> 00:05:15.980
its weights and biases that it gets

00:05:15.980 --> 00:05:17.765
from the output of the layer before,

00:05:17.765 --> 00:05:21.020
and so on, all the way back to the original input layers,

00:05:21.020 --> 00:05:23.180
values, weights, and biases.

00:05:23.180 --> 00:05:24.980
In effect, the neural network and

00:05:24.980 --> 00:05:28.075
its lost function just form one big nested function.

00:05:28.075 --> 00:05:30.320
Like any function, we can plot

00:05:30.320 --> 00:05:33.630
its output for any combination of its inputs.

00:05:33.790 --> 00:05:36.445
So to keep things simple,

00:05:36.445 --> 00:05:37.950
let's for a moment pretend that

00:05:37.950 --> 00:05:40.320
the lost function is only one input value,

00:05:40.320 --> 00:05:42.750
a weight value named w_1,

00:05:42.750 --> 00:05:46.895
and it produces an output which measures the loss in our network.

00:05:46.895 --> 00:05:50.060
Here's the point on the plot that shows

00:05:50.060 --> 00:05:54.000
the loss generated for w_1, value of two.

00:05:54.000 --> 00:05:57.080
Using differential calculus, we can

00:05:57.080 --> 00:05:59.870
calculate the derivative of the function at this point.

00:05:59.870 --> 00:06:02.870
In other words, we can figure out the direction of the slope

00:06:02.870 --> 00:06:06.335
or gradient of the function line when w_1 is two.

00:06:06.335 --> 00:06:09.170
In this case, the derivative is negative.

00:06:09.170 --> 00:06:12.485
So we know that if we increase the value of w_2,

00:06:12.485 --> 00:06:16.710
then the gradient of the function will go down reducing the loss.

00:06:16.990 --> 00:06:20.600
Now of course, our function is a little more complex.

00:06:20.600 --> 00:06:22.745
It has many input variables,

00:06:22.745 --> 00:06:26.075
but the same principle applies to a multivariate function.

00:06:26.075 --> 00:06:28.925
It's just that the plot becomes multidimensional,

00:06:28.925 --> 00:06:31.984
and we need to calculate a partial derivative

00:06:31.984 --> 00:06:34.135
for each input variable.

00:06:34.135 --> 00:06:36.905
This tells us which direction we need to adjust

00:06:36.905 --> 00:06:39.575
each variable in order to move the loss down,

00:06:39.575 --> 00:06:41.420
in effect descending the gradient

00:06:41.420 --> 00:06:43.895
of the function to reduce the loss.

00:06:43.895 --> 00:06:48.460
The amount we adjust each variable by is called the learning rate.

00:06:48.460 --> 00:06:52.295
A low value for the learning rate results in small adjustments.

00:06:52.295 --> 00:06:54.080
So we might need to adjust the variables

00:06:54.080 --> 00:06:56.900
many times to minimize the loss.

00:06:56.900 --> 00:07:00.935
A high learning rate results in large adjustments.

00:07:00.935 --> 00:07:02.630
So we might step too far,

00:07:02.630 --> 00:07:04.970
and not find the minimum part of the gradient.

00:07:04.970 --> 00:07:07.430
In practice, setting the learning rate

00:07:07.430 --> 00:07:08.820
require some trial and error,

00:07:08.820 --> 00:07:10.760
and you can use advanced techniques to change

00:07:10.760 --> 00:07:14.400
the learning rate as we get closer to minimizing the loss.

00:07:15.020 --> 00:07:17.435
We can calculate derivatives for

00:07:17.435 --> 00:07:19.285
any of the variables in the functions.

00:07:19.285 --> 00:07:22.130
Thanks to a feature of calculus called the chain rule,

00:07:22.130 --> 00:07:24.170
we can work recursively back through

00:07:24.170 --> 00:07:26.330
any number of nested functions to calculate

00:07:26.330 --> 00:07:28.280
derivatives for every variable that

00:07:28.280 --> 00:07:31.370
affects the final outcome, in this case the loss.

00:07:31.370 --> 00:07:34.295
So if we apply this to our neural network,

00:07:34.295 --> 00:07:38.390
we can work backwards to determine how best to adjust

00:07:38.390 --> 00:07:40.040
every weight and bias value in

00:07:40.040 --> 00:07:43.280
the network in order to reduce the amount of loss in the model,

00:07:43.280 --> 00:07:45.730
we call this backpropagation.

00:07:45.730 --> 00:07:48.745
With the weights and biases adjusted,

00:07:48.745 --> 00:07:51.740
when we feed the same input values into the network,

00:07:51.740 --> 00:07:54.020
we get a different output which

00:07:54.020 --> 00:07:57.305
should hopefully result in a lower loss.

00:07:57.305 --> 00:08:01.630
Which means our network is improving, it's learning.

00:08:02.660 --> 00:08:05.070
To train our network,

00:08:05.070 --> 00:08:06.520
we just feed through a bunch of

00:08:06.520 --> 00:08:09.685
feature vectors for which we already know the class labels.

00:08:09.685 --> 00:08:11.830
Then our loss function calculates

00:08:11.830 --> 00:08:14.155
the loss averaged for all of the training data,

00:08:14.155 --> 00:08:16.150
and we backpropagate, to adjust

00:08:16.150 --> 00:08:19.465
the weights and bias values based on our chosen learning rate.

00:08:19.465 --> 00:08:22.450
Then we can feed through some more feature vectors

00:08:22.450 --> 00:08:26.020
with known labels to validate the new weights and biases.

00:08:26.020 --> 00:08:29.260
Each cycle of training backpropagation

00:08:29.260 --> 00:08:31.240
and validation is called an epoch,

00:08:31.240 --> 00:08:34.820
and we go through several of these to train the model.

00:08:35.720 --> 00:08:38.500
Within each epoch, we don't feed

00:08:38.500 --> 00:08:40.380
the feature vectors through one at a time.

00:08:40.380 --> 00:08:42.800
We actually combine them into multiple batches.

00:08:42.800 --> 00:08:46.535
So actually, we're processing a matrix of feature values.

00:08:46.535 --> 00:08:49.730
The weights and biases are vectors.

00:08:49.730 --> 00:08:54.125
So this process is actually just some linear algebra calculations.

00:08:54.125 --> 00:08:57.515
Now that kind of math is heavily used in computer graphics.

00:08:57.515 --> 00:08:59.495
Graphical processing units or

00:08:59.495 --> 00:09:03.230
GPUs are optimized for this type of operation.

00:09:03.230 --> 00:09:06.440
That's why when we're training deep learning models,

00:09:06.440 --> 00:09:08.290
we tend to use computers with GPUs.

00:09:08.290 --> 00:09:12.270
It's generally much faster than using just CPUs.
