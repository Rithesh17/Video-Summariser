WEBVTT - https://subtitletools.com

00:00:01.130 --> 00:00:05.190
>> Classification is a form of supervised machine learning,

00:00:05.190 --> 00:00:06.780
in which we train a model to determine

00:00:06.780 --> 00:00:10.890
the categorization or class of some item of interest.

00:00:10.890 --> 00:00:14.190
So does that actually mean? Well, it helps to

00:00:14.190 --> 00:00:17.775
think of machine learning models as mathematical functions.

00:00:17.775 --> 00:00:20.550
A function that operates on an input value,

00:00:20.550 --> 00:00:22.080
which we'll call x, and

00:00:22.080 --> 00:00:25.290
calculates an output value, which we'll call y.

00:00:25.290 --> 00:00:27.839
In machine learning terminology,

00:00:27.839 --> 00:00:31.800
x represents the features of the item that we're interested in.

00:00:31.800 --> 00:00:35.850
These are characteristics of the item usually numeric values.

00:00:35.850 --> 00:00:39.180
The y represents a label,

00:00:39.180 --> 00:00:41.145
the thing that we want to predict.

00:00:41.145 --> 00:00:44.655
In this case, that's the class of the object.

00:00:44.655 --> 00:00:47.810
Okay. Well, that all seems a bit abstract.

00:00:47.810 --> 00:00:49.310
So let's look at what's probably

00:00:49.310 --> 00:00:51.860
the quintessential example of classification,

00:00:51.860 --> 00:00:54.295
the Hello World of Machine Learning.

00:00:54.295 --> 00:00:59.390
In 1936, British statistician, Ronald Fisher,

00:00:59.390 --> 00:01:01.100
introduced a paper in which he

00:01:01.100 --> 00:01:02.810
described how to determine the species

00:01:02.810 --> 00:01:07.435
of iris flower based on the measurements of its sepals and petals.

00:01:07.435 --> 00:01:11.560
In this scenario, the features our x value,

00:01:11.560 --> 00:01:14.930
is a vector containing the height and width of the sepals and

00:01:14.930 --> 00:01:19.055
petals for the observed plants. Here's an example.

00:01:19.055 --> 00:01:22.790
The label, the y value we're trying to predict,

00:01:22.790 --> 00:01:27.395
is the species of iris of which there are three possible values;

00:01:27.395 --> 00:01:30.445
Setosa, Versicolor, and Virginica.

00:01:30.445 --> 00:01:32.400
We'll assign the values zero,

00:01:32.400 --> 00:01:34.810
one, and two to these classes.

00:01:34.810 --> 00:01:36.920
The y value generated by

00:01:36.920 --> 00:01:39.935
the function is a vector of probabilities,

00:01:39.935 --> 00:01:42.620
one for each of the possible classes.

00:01:42.620 --> 00:01:44.880
So in this example, the function predicts

00:01:44.880 --> 00:01:46.410
a zero percent probability of

00:01:46.410 --> 00:01:48.420
the first and third classes which are

00:01:48.420 --> 00:01:50.415
labeled zero and two and

00:01:50.415 --> 00:01:51.890
100 percent probability for

00:01:51.890 --> 00:01:54.370
the second class, which we labeled one.

00:01:54.370 --> 00:01:56.814
So in this particular example,

00:01:56.814 --> 00:02:00.540
this flower is an example of a versicolor.

00:02:00.820 --> 00:02:03.470
So how did we define the function that

00:02:03.470 --> 00:02:05.750
calculates this probability vector?

00:02:05.750 --> 00:02:08.330
Well, we start with some historical observations of

00:02:08.330 --> 00:02:11.780
data where we already know the correct label values.

00:02:11.780 --> 00:02:15.665
In this case, iris measurements where we know the species.

00:02:15.665 --> 00:02:17.180
This dataset gives us

00:02:17.180 --> 00:02:20.880
some ground truth with which we can train a function.

00:02:21.610 --> 00:02:23.825
Now, we take some,

00:02:23.825 --> 00:02:25.790
but not all of our data and

00:02:25.790 --> 00:02:28.430
apply an algorithm such as logistic regression,

00:02:28.430 --> 00:02:30.890
which calculates coefficients that we can

00:02:30.890 --> 00:02:33.170
use to map the observation vectors into

00:02:33.170 --> 00:02:35.390
multidimensional space within which that are

00:02:35.390 --> 00:02:39.540
boundaries or thresholds that separate the different classes.

00:02:39.940 --> 00:02:43.910
Then we take the data that we held back and we use it to

00:02:43.910 --> 00:02:47.555
validate the model we've trained by applying the function to it.

00:02:47.555 --> 00:02:52.670
Ideally, it should assign each observation to the right class.

00:02:52.670 --> 00:02:56.810
We can then tabulate the results from our validation data into

00:02:56.810 --> 00:02:58.910
a structure called a confusion matrix

00:02:58.910 --> 00:03:00.590
which counts the predictions,

00:03:00.590 --> 00:03:04.990
intersecting the predicted classes with the actual known classes.

00:03:04.990 --> 00:03:07.420
In an ideal model,

00:03:07.420 --> 00:03:09.920
there should be a high number of cases where

00:03:09.920 --> 00:03:12.800
the predicted class matches the actual class,

00:03:12.800 --> 00:03:16.530
which shows up as a diagonal trend in the confusion matrix.
